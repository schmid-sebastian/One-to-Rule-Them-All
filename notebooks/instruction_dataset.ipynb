{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "instruction = \"\"\"Below is an excerpt from an event log used in process mining. Each row represents an event and contains an event ID, case ID, activity, and timestamp. The event log may contain one or multiple event log imperfection patterns which may affect the case ID, activity, and/or timestamp attribute.\n",
    "\n",
    "Your task is to:\n",
    "1. Diagnose the event log by identifying imperfection patterns. Clearly state which issues you detected and explain how you identified them.\n",
    "2. Propose a correction strategy for each detected issue you will follow to mitigate the imperfections.\n",
    "3. Repair the event log and output the corrected version in the same format as above.\n",
    "\n",
    "Structure your response as follows:\n",
    "<diagnosis>\n",
    "[List the detected imperfections with explanations]\n",
    "</diagnosis>\n",
    "\n",
    "<mitigation>\n",
    "[Describe the correction strategies]\n",
    "</mitigation>\n",
    "\n",
    "<log>\n",
    "[Provide the repaired event log]\n",
    "</log>\"\"\"\n",
    "\n",
    "### SPLIT EVENT LOG INTO CHUNKS ###\n",
    "def split_event_log(df, batch_size=100, soft_limit=150):\n",
    "    \"\"\"\n",
    "    Splits event log into batches while ensuring full traces (cases) are not broken.\n",
    "    Ensures each case appears at least once.\n",
    "    \"\"\"\n",
    "    case_ids = df[\"case_id\"].unique().tolist()\n",
    "    random.shuffle(case_ids)  # Shuffle case IDs to ensure randomness\n",
    "\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_size = 0\n",
    "\n",
    "    for case_id in case_ids:\n",
    "        case_events = df[df[\"case_id\"] == case_id]\n",
    "        case_size = len(case_events)\n",
    "\n",
    "        if current_size + case_size > soft_limit and current_batch:\n",
    "            batches.append(pd.concat(current_batch))\n",
    "            current_batch = []\n",
    "            current_size = 0\n",
    "\n",
    "        current_batch.append(case_events)\n",
    "        current_size += case_size\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(pd.concat(current_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def apply_random_imperfections(df, patterns, num_samples=10):\n",
    "    \"\"\"\n",
    "    Applies the selected patterns in random combinations of error rates to the dataframe.\n",
    "    Only non-None patterns are applied so that each valid pattern is injected once per combination.\n",
    "    Returns a list of tuples: (erroneous_df, applied_patterns) for each combination.\n",
    "    \"\"\"\n",
    "    # Define available error rates (10% to 100%)\n",
    "    error_rates = list(range(10, 110, 10))\n",
    "    \n",
    "    # Filter out None patterns so only valid ones are used for combinations.\n",
    "    valid_patterns = [p for p in patterns if p is not None]\n",
    "    \n",
    "    # Create the full list of all combinations of error rates for the valid patterns.\n",
    "    all_combinations = list(product(error_rates, repeat=len(valid_patterns)))\n",
    "    \n",
    "    # Randomly sample up to num_samples combinations from the full list.\n",
    "    sampled_combinations = random.sample(all_combinations, min(num_samples, len(all_combinations)))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for rates in sampled_combinations:\n",
    "        erroneous_df = df.copy()\n",
    "        applied_patterns = {}\n",
    "        \n",
    "        for pattern, error_rate in zip(valid_patterns, rates):\n",
    "            erroneous_df, affected_event_ids = pattern(erroneous_df, error_rate)\n",
    "            pattern_name = pattern.__name__.replace('inject_', '').replace('_', ' ')\n",
    "            applied_patterns[pattern_name] = {\n",
    "                'error_rate': error_rate,\n",
    "                'affected_ids': affected_event_ids\n",
    "            }\n",
    "        \n",
    "        results.append((erroneous_df, applied_patterns))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def inject_form_based_event_capture(df, error_rate):\n",
    "    \"\"\"Injects the form-based event capture pattern by assigning a shared timestamp to selected events, shuffling the dataset, and restoring order.\"\"\"\n",
    "    df = df.copy()  # Ensure we don't modify the original DataFrame\n",
    "    affected_events = df.sample(frac=error_rate / 100).index  # Select X% of all events\n",
    "    affected_event_ids = []\n",
    "    \n",
    "    for idx in affected_events:\n",
    "        case_id = df.at[idx, \"case_id\"]\n",
    "        case_events = df[df[\"case_id\"] == case_id]\n",
    "\n",
    "        if len(case_events) > 1:\n",
    "            # Choose a random timestamp from another event in the same case\n",
    "            shared_timestamp_event = case_events.sample(n=1).iloc[0]\n",
    "            df.at[idx, \"timestamp\"] = shared_timestamp_event[\"timestamp\"]\n",
    "            affected_event_ids.append(df.at[idx, \"event_id\"])\n",
    "            affected_event_ids.append(shared_timestamp_event[\"event_id\"])\n",
    "\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Restore order by case_id and timestamp\n",
    "    df = df.sort_values(by=[\"case_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    return df, affected_event_ids\n",
    "\n",
    "\n",
    "\n",
    "def inject_unanchored_event(df, error_rate):\n",
    "    \"\"\"Injects inconsistent timestamp formats.\"\"\"\n",
    "    formats = [\"%Y-%m-%d %H:%M:%S %z\", \"%m/%d/%Y %H:%M %z\", \"%d.%m.%Y %H:%M:%S %z\"]\n",
    "    affected_events = df.sample(frac=error_rate / 100).index\n",
    "    affected_event_ids = []\n",
    "\n",
    "    for idx in affected_events:\n",
    "        original_timestamp = df.at[idx, \"timestamp\"]\n",
    "        \n",
    "        # Ensure it's a proper datetime object\n",
    "        if isinstance(original_timestamp, str):\n",
    "            try:\n",
    "                original_timestamp = pd.to_datetime(original_timestamp, utc=True)  # Convert with timezone awareness\n",
    "            except Exception:\n",
    "                continue  # Skip if conversion fails\n",
    "\n",
    "        if isinstance(original_timestamp, pd.Timestamp):\n",
    "            timezone_info = original_timestamp.tz  # Extract timezone\n",
    "\n",
    "            # Format the timestamp into a new format (keeping timezone)\n",
    "            new_format = random.choice(formats)\n",
    "            new_timestamp_str = original_timestamp.strftime(new_format)\n",
    "\n",
    "            # Convert back to datetime with the same timezone\n",
    "            df.at[idx, \"timestamp\"] = new_timestamp_str\n",
    "            affected_event_ids.append(df.at[idx, \"event_id\"])\n",
    "                                      \n",
    "    return df, affected_event_ids\n",
    "\n",
    "\n",
    "def inject_collateral_events(df, error_rate):\n",
    "    \"\"\"DISCONTINUED.\"\"\"\n",
    "    affected_cases = df[\"case_id\"].unique()\n",
    "    num_affected = int(len(affected_cases) * (error_rate / 100))\n",
    "    selected_cases = random.sample(list(affected_cases), num_affected)\n",
    "\n",
    "    for case in selected_cases:\n",
    "        case_events = df[df[\"case_id\"] == case]\n",
    "        num_duplicates = int(len(case_events) * (error_rate / 100))\n",
    "\n",
    "        for _ in range(num_duplicates):\n",
    "            duplicate_event = case_events.sample(n=1).copy()\n",
    "            duplicate_event[\"timestamp\"] += timedelta(seconds=random.randint(1, 5))\n",
    "            df = pd.concat([df, duplicate_event])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def inject_elusive_case(df, error_rate):\n",
    "    \"\"\"Removes case IDs from some events.\"\"\"\n",
    "    affected_events = df.sample(frac=error_rate / 100).index\n",
    "    df.loc[affected_events, \"case_id\"] = None  # Remove case IDs\n",
    "    affected_event_ids = df.loc[affected_events, \"event_id\"].to_list()\n",
    "    return df, affected_event_ids\n",
    "\n",
    "\n",
    "def inject_polluted_labels(df, error_rate):\n",
    "    \"\"\"Adds random strings to activity labels.\"\"\"\n",
    "    affected_events = df.sample(frac=error_rate / 100).index\n",
    "    affected_event_ids = []\n",
    "    for idx in affected_events:\n",
    "        df.at[idx, \"activity\"] += \" - \" + str(random.randint(1000, 9999))\n",
    "        affected_event_ids.append(df.at[idx, \"event_id\"])\n",
    "        \n",
    "    return df, affected_event_ids\n",
    "\n",
    "\n",
    "def inject_distorted_label(df, error_rate):\n",
    "    \"\"\"Swaps two letters in activity labels.\"\"\"\n",
    "    affected_events = df.sample(frac=error_rate / 100).index\n",
    "    affected_event_ids = []\n",
    "    for idx in affected_events:\n",
    "        activity = df.at[idx, \"activity\"]\n",
    "        if len(activity) > 3:\n",
    "            i = random.randint(0, len(activity) - 2)\n",
    "            activity = activity[:i] + activity[i+1] + activity[i] + activity[i+2:]\n",
    "        df.at[idx, \"activity\"] = activity\n",
    "        affected_event_ids.append(df.at[idx, \"event_id\"])\n",
    "\n",
    "    return df, affected_event_ids\n",
    "\n",
    "def descriptive_output(applied_patterns):\n",
    "    \"\"\"\n",
    "    Write the diagnosis and mitigation part of the instruction dataset.\n",
    "    \"\"\"\n",
    "    diagnosis = \"<diagnosis>\\n\"\n",
    "    mitigation = \"<mitigation>\\n\"\n",
    "    \n",
    "    for pattern, details in applied_patterns.items():\n",
    "        \n",
    "        affected_ids = details['affected_ids']\n",
    "        error_rate = str(details['error_rate']) + \"%\"\n",
    "        \n",
    "        diagnosis += f\"Detected {pattern}: {len(affected_ids)} affected events which corresponds to an error rate of {error_rate}. \"\n",
    "        \n",
    "        if pattern == \"form based event capture\":\n",
    "            diagnosis += \"This pattern was detected because some events have identical timestamps as other events in their case. \"\n",
    "            #diagnosis += f\"The following events were affected: {' '.join(affected_ids)}.\\n\"\n",
    "            mitigation += f\"To mitigate the {pattern}, I will adjust the erroneous timestamps based on typical activity duration in correct data samples.\\n\"\n",
    "        \n",
    "        elif pattern == \"unanchored event\":\n",
    "            diagnosis += \"This pattern was detected due to inconsistencies in timestamp formatting, such as variations in date-month or month-date format. \"\n",
    "            #diagnosis += f\"The following events were affected: {' '.join(affected_ids)}.\\n\"\n",
    "            mitigation += f\"To mitigate the {pattern}, I will standardize all timestamps to a uniform format.\\n\"\n",
    "        \n",
    "        elif pattern == \"elusive case\":\n",
    "            diagnosis += \"This pattern was detected because some events are missing case IDs. \"\n",
    "            #diagnosis += f\"The following events were affected: {' '.join(affected_ids)}.\\n\"\n",
    "            mitigation += f\"To mitigate the {pattern}, I will assign missing case IDs based on time proximity and missing activities in known cases.\\n\"\n",
    "        \n",
    "        elif pattern == \"polluted labels\":\n",
    "            diagnosis += \"This pattern was detected because activity labels contain extraneous identifiers, making them inconsistent. \"\n",
    "            #diagnosis += f\"The following events were affected: {' '.join(affected_ids)}.\\n\"\n",
    "            mitigation += f\"To mitigate the {pattern}, I will extract and retain the core activity names, removing unnecessary elements.\\n\"\n",
    "        \n",
    "        elif pattern == \"distorted label\":\n",
    "            diagnosis += \"This pattern was detected because activity labels contain minor spelling errors or variations. \"\n",
    "            #diagnosis += f\"The following events were affected: {' '.join(affected_ids)}.\\n\"\n",
    "            mitigation += f\"To mitigate the {pattern}, I will standardize activity labels to their predominant correct form.\\n\"\n",
    "        \n",
    "    diagnosis += \"</diagnosis>\\n\\n\"\n",
    "    mitigation += \"</mitigation>\\n\\n\"\n",
    "    \n",
    "    return diagnosis + mitigation\n",
    "\n",
    "\n",
    "def dataframe_to_csv_string(df):\n",
    "    \"\"\"Converts a DataFrame to a CSV-like string (without saving as a file).\"\"\"\n",
    "    return df.to_csv(index=False)\n",
    "\n",
    "\n",
    "def save_to_json(original_df, erroneous_df, applied_patterns, filename):\n",
    "    \"\"\"Saves event logs in JSON format with CSV-like string representation.\"\"\"\n",
    "    output_string = descriptive_output(applied_patterns) + \"<log>\\n\" + dataframe_to_csv_string(original_df) + \"\\n</log>\"\n",
    "    \n",
    "    output_data = [{\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": dataframe_to_csv_string(erroneous_df),\n",
    "        \"output\": output_string,\n",
    "    }]\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "        \n",
    "        \n",
    "def generate_data(input_dir, output_dir):\n",
    "    \"\"\"Generates a full set of instruction data.\"\"\"\n",
    "    for file in os.listdir(input_dir):\n",
    "        print(\"CURRENTLY AT\", file)\n",
    "        # use dictionary comprehension to make dict of dtypes\n",
    "        dict_dtypes = {x : 'str'  for x in ['case_id', 'event_id', 'activity']}  \n",
    "        df = pd.read_csv(input_dir + file, dtype=dict_dtypes)\n",
    "        \n",
    "        batches = split_event_log(df)\n",
    "        \n",
    "        # Define available imperfection patterns\n",
    "        timestamp_patterns = [inject_form_based_event_capture, inject_unanchored_event]\n",
    "        case_id_patterns = [inject_elusive_case]\n",
    "        label_patterns = [inject_polluted_labels, inject_distorted_label]\n",
    "\n",
    "        # Generate all valid combinations (each group contributes at most one pattern)\n",
    "        all_valid_combinations = list(product(\n",
    "            [None] + timestamp_patterns,  # None means this group is not selected\n",
    "            [None] + case_id_patterns,\n",
    "            [None] + label_patterns\n",
    "        ))\n",
    "\n",
    "        # Remove the (None, None, None) combination (no patterns applied)\n",
    "        all_valid_combinations.remove((None, None, None))\n",
    "        \n",
    "        for i, batch in tqdm(enumerate(batches)):\n",
    "            j = 0\n",
    "            for pattern in all_valid_combinations:\n",
    "                results = apply_random_imperfections(batch, pattern)\n",
    "                for erroneous_batch, applied_patterns in results:\n",
    "                    save_to_json(batch, erroneous_batch, applied_patterns, f\"{output_dir}{file}_batch_{i}_combination_{j}.json\")\n",
    "                    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENTLY AT 2020_RequestForPayment.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [31:48,  9.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENTLY AT 2012_BPI_Challenge.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "293it [41:39,  8.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m TRAIN_OUTPUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/instruction/train/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m TEST_OUTPUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/instruction/test/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_OUTPUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m generate_data(TEST_DIR, TEST_OUTPUT)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mgenerate_data\u001b[0;34m(input_dir, output_dir)\u001b[0m\n\u001b[1;32m    295\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m all_valid_combinations:\n\u001b[0;32m--> 297\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mapply_random_imperfections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m erroneous_batch, applied_patterns \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m    299\u001b[0m         save_to_json(batch, erroneous_batch, applied_patterns, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_combination_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mapply_random_imperfections\u001b[0;34m(df, patterns, num_samples)\u001b[0m\n\u001b[1;32m     82\u001b[0m applied_patterns \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern, error_rate \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(valid_patterns, rates):\n\u001b[0;32m---> 85\u001b[0m     erroneous_df, affected_event_ids \u001b[38;5;241m=\u001b[39m \u001b[43mpattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43merroneous_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     pattern_name \u001b[38;5;241m=\u001b[39m pattern\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minject_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     87\u001b[0m     applied_patterns[pattern_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: error_rate,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffected_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: affected_event_ids\n\u001b[1;32m     90\u001b[0m     }\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36minject_unanchored_event\u001b[0;34m(df, error_rate)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(original_timestamp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         original_timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_timestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert with timezone awareness\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if conversion fails\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:1101\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         result \u001b[38;5;241m=\u001b[39m convert_listlike(argc, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mbool_):\n\u001b[1;32m   1103\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(result)  \u001b[38;5;66;03m# TODO: avoid this kludge.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:429\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    426\u001b[0m arg \u001b[38;5;241m=\u001b[39m ensure_object(arg)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_guess_datetime_format_for_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:131\u001b[0m, in \u001b[0;36m_guess_datetime_format_for_array\u001b[0;34m(arr, dayfirst)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (first_non_null \u001b[38;5;241m:=\u001b[39m tslib\u001b[38;5;241m.\u001b[39mfirst_non_null(arr)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first_non_nan_element \u001b[38;5;241m:=\u001b[39m arr[first_non_null]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# GH#32264 np.str_ object\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m         guessed_format \u001b[38;5;241m=\u001b[39m \u001b[43mguess_datetime_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfirst_non_nan_element\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m guessed_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m guessed_format\n",
      "File \u001b[0;32mparsing.pyx:1013\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.guess_datetime_format\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing._fill_token\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/re.py:201\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msearch(string)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/re.py:291\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compile\u001b[39m(pattern, flags):\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# internal: compile pattern\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRegexFlag\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    292\u001b[0m         flags \u001b[38;5;241m=\u001b[39m flags\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Input paths\n",
    "TRAIN_DIR = \"../data/train/\"\n",
    "TEST_DIR = \"../data/test/\"\n",
    "\n",
    "# Output paths\n",
    "TRAIN_OUTPUT = \"../data/instruction/train/\"\n",
    "TEST_OUTPUT = \"../data/instruction/test/\"\n",
    "\n",
    "generate_data(TRAIN_DIR, TRAIN_OUTPUT)\n",
    "generate_data(TEST_DIR, TEST_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
