{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26e75a-5bc8-4b68-969a-d0fa89cdae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def load_json_files(directory):\n",
    "    \"\"\"Load all JSON files from a directory and return a dictionary with filenames as keys.\"\"\"\n",
    "    test_files = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                test_files[filename] = json.load(file)  # Store data with filename as key\n",
    "    return test_files\n",
    "\n",
    "def extract_tag_content(text, tag):\n",
    "    \"\"\"Extracts the substring between the last occurrence of <tag> and </tag> from the given text.\"\"\"\n",
    "    pattern = fr'<{tag}>(.*?)</{tag}>'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[-1].strip() if matches else None\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing punctuation and converting to lowercase.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text).lower()\n",
    "\n",
    "def find_patterns(text, patterns):\n",
    "    \"\"\"\n",
    "    Find which patterns are in the text, ignoring case and punctuation.\n",
    "    Returns a list of matched patterns.\n",
    "    \"\"\"\n",
    "    normalized_text = normalize_text(text)\n",
    "    matched_patterns = [pattern for pattern in patterns if normalize_text(pattern) in normalized_text]\n",
    "    return matched_patterns\n",
    "\n",
    "def find_error_rate(text, patterns):\n",
    "    \"\"\"Find error rates for given patterns in the text.\"\"\"\n",
    "    #normalized_text = normalize_text(text)\n",
    "    results = {}\n",
    "    for pattern in patterns:\n",
    "        normalized_pattern = normalize_text(pattern)\n",
    "        match = re.search(fr'{normalized_pattern}.*?error rate of (\\d+)%', text)\n",
    "        if match:\n",
    "            results[pattern] = int(match.group(1))\n",
    "    return results\n",
    "\n",
    "def read_csv_from_text(text):\n",
    "    data = StringIO(text)\n",
    "    df = pd.read_csv(data)\n",
    "    return df\n",
    "\n",
    "def evaluate_diagnosis(true_diagnosis, predicted_diagnosis):\n",
    "    \"\"\"Evaluate how well the LLM identified imperfection patterns in text.\"\"\"\n",
    "    patterns = [\"form-based event capture\", \"elusive case\", \"polluted label\", \"distorted label\", \"unanchored event\"]\n",
    "\n",
    "    # Find detected patterns in both true and predicted diagnoses\n",
    "    true_detected = set(find_patterns(true_diagnosis, patterns))\n",
    "    pred_detected = set(find_patterns(predicted_diagnosis, patterns))\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    tp = len(true_detected & pred_detected)\n",
    "    fp = len(pred_detected - true_detected)\n",
    "    fn = len(true_detected - pred_detected)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}, true_detected\n",
    "\n",
    "def timestamp_deviation(df_erroneous, df_true, df_repaired):\n",
    "    \"\"\"Calculate the mean deviation of repaired timestamps from ground truth in seconds (only for erroneous events).\"\"\"\n",
    "    merged_err = df_erroneous.merge(df_true, on=\"event_id\", suffixes=(\"_err\", \"_true\"))\n",
    "    erroneous_events = merged_err[merged_err[\"timestamp_err\"] != merged_err[\"timestamp_true\"]][\"event_id\"]\n",
    "    df_true_filtered = df_true[df_true[\"event_id\"].isin(erroneous_events)]\n",
    "    df_repaired_filtered = df_repaired[df_repaired[\"event_id\"].isin(erroneous_events)]\n",
    "    merged = df_true_filtered.merge(df_repaired_filtered, on=\"event_id\", suffixes=(\"_true\", \"_repaired\"))\n",
    "    merged[\"timestamp_true\"] = pd.to_datetime(merged[\"timestamp_true\"], errors='coerce')\n",
    "    merged[\"timestamp_repaired\"] = pd.to_datetime(merged[\"timestamp_repaired\"], errors='coerce')\n",
    "    merged[\"deviation\"] = (merged[\"timestamp_repaired\"] - merged[\"timestamp_true\"]).dt.total_seconds().abs()\n",
    "    return merged[\"deviation\"].mean()\n",
    "\n",
    "def timestamp_accuracy(df_erroneous, df_true, df_repaired):\n",
    "    \"\"\"Calculate the accuracy of perfectly corrected timestamps using string comparison (only for erroneous events).\"\"\"\n",
    "    merged_err = df_erroneous.merge(df_true, on=\"event_id\", suffixes=(\"_err\", \"_true\"))\n",
    "    erroneous_events = merged_err[merged_err[\"timestamp_err\"] != merged_err[\"timestamp_true\"]][\"event_id\"]\n",
    "    df_true_filtered = df_true[df_true[\"event_id\"].isin(erroneous_events)]\n",
    "    df_repaired_filtered = df_repaired[df_repaired[\"event_id\"].isin(erroneous_events)]\n",
    "    merged = df_true_filtered.merge(df_repaired_filtered, on=\"event_id\", suffixes=(\"_true\", \"_repaired\"))\n",
    "    correct_timestamps = (merged[\"timestamp_true\"].astype(str) == merged[\"timestamp_repaired\"].astype(str)).sum()\n",
    "    total_timestamps = len(merged)\n",
    "    return correct_timestamps / total_timestamps if total_timestamps > 0 else 0\n",
    "\n",
    "def case_id_accuracy(df_erroneous, df_true, df_repaired):\n",
    "    \"\"\"Calculate the accuracy of restored case IDs (only for erroneous events).\"\"\"\n",
    "    merged_err = df_erroneous.merge(df_true, on=\"event_id\", suffixes=(\"_err\", \"_true\"))\n",
    "    erroneous_events = merged_err[merged_err[\"case_id_err\"] != merged_err[\"case_id_true\"]][\"event_id\"]\n",
    "    df_true_filtered = df_true[df_true[\"event_id\"].isin(erroneous_events)]\n",
    "    df_repaired_filtered = df_repaired[df_repaired[\"event_id\"].isin(erroneous_events)]\n",
    "    merged = df_true_filtered.merge(df_repaired_filtered, on=\"event_id\", suffixes=(\"_true\", \"_repaired\"))\n",
    "    correct_cases = (merged[\"case_id_true\"] == merged[\"case_id_repaired\"]).sum()\n",
    "    total_cases = len(merged)\n",
    "    return correct_cases / total_cases if total_cases > 0 else 0\n",
    "\n",
    "def label_accuracy(df_erroneous, df_true, df_repaired):\n",
    "    \"\"\"Calculate the accuracy of restored activity labels (only for erroneous events).\"\"\"\n",
    "    merged_err = df_erroneous.merge(df_true, on=\"event_id\", suffixes=(\"_err\", \"_true\"))\n",
    "    erroneous_events = merged_err[merged_err[\"activity_err\"] != merged_err[\"activity_true\"]][\"event_id\"]\n",
    "    df_true_filtered = df_true[df_true[\"event_id\"].isin(erroneous_events)]\n",
    "    df_repaired_filtered = df_repaired[df_repaired[\"event_id\"].isin(erroneous_events)]\n",
    "    merged = df_true_filtered.merge(df_repaired_filtered, on=\"event_id\", suffixes=(\"_true\", \"_repaired\"))\n",
    "    correct_labels = (merged[\"activity_true\"] == merged[\"activity_repaired\"]).sum()\n",
    "    total_labels = len(merged)\n",
    "    return correct_labels / total_labels if total_labels > 0 else 0\n",
    "\n",
    "def evaluate_log_repairs(df_erroneous, df_true, df_repaired, true_patterns):\n",
    "    \"\"\"Evaluate how well the log was repaired based on detected patterns.\"\"\"\n",
    "    metrics = {}\n",
    "    if \"form-based event capture\" in true_patterns:\n",
    "        metrics[\"timestamp_deviation\"] = timestamp_deviation(df_erroneous, df_true, df_repaired)\n",
    "    if \"unanchored event\" in true_patterns:\n",
    "        metrics[\"timestamp_accuracy\"] = timestamp_accuracy(df_erroneous, df_true, df_repaired)\n",
    "    if \"elusive case\" in true_patterns:\n",
    "        metrics[\"case_id_accuracy\"] = case_id_accuracy(df_erroneous, df_true, df_repaired)\n",
    "    if \"polluted label\" in true_patterns:\n",
    "        metrics[\"polluted_label_accuracy\"] = label_accuracy(df_erroneous, df_true, df_repaired)\n",
    "    if \"distorted label\" in true_patterns:\n",
    "        metrics[\"distorted_label_accuracy\"] = label_accuracy(df_erroneous, df_true, df_repaired)\n",
    "    return metrics\n",
    "\n",
    "def get_failure_template(ground_truth_diagnosis):\n",
    "    patterns = [\"form-based event capture\", \"elusive case\", \"polluted label\", \"distorted label\", \"unanchored event\"]\n",
    "    injected = set(find_patterns(ground_truth_diagnosis, patterns))\n",
    "    repair_metrics = {}\n",
    "    if \"form-based event capture\" in injected:\n",
    "        repair_metrics[\"timestamp_deviation\"] = 0.0\n",
    "    if \"unanchored event\" in injected:\n",
    "        repair_metrics[\"timestamp_accuracy\"] = 0.0\n",
    "    if \"elusive case\" in injected:\n",
    "        repair_metrics[\"case_id_accuracy\"] = 0.0\n",
    "    if \"polluted label\" in injected:\n",
    "        repair_metrics[\"polluted_label_accuracy\"] = 0.0\n",
    "    if \"distorted label\" in injected:\n",
    "        repair_metrics[\"distorted_label_accuracy\"] = 0.0\n",
    "    \n",
    "    failure_template = {\n",
    "        \"diagnosis\": {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0},\n",
    "        \"patterns_injected\": list(injected),\n",
    "        \"repair\": repair_metrics\n",
    "    }\n",
    "    \n",
    "    return failure_template\n",
    "      \n",
    "\n",
    "def evaluate_model(test_files, skip_errors=True):\n",
    "    \"\"\"Run evaluation on all test files (provided as a dict keyed by filename) and return a dict with filenames as keys.\"\"\"\n",
    "    results = {}\n",
    "    for filename, test_file in test_files.items():\n",
    "        # Extract ground truth data\n",
    "        erroneous_log = test_file[\"input\"]\n",
    "        df_erroneous = read_csv_from_text(erroneous_log)\n",
    "        ground_truth_log = extract_tag_content(test_file[\"output\"], \"log\")\n",
    "        df_true = read_csv_from_text(ground_truth_log)\n",
    "        ground_truth_diagnosis = extract_tag_content(test_file[\"output\"], \"diagnosis\")\n",
    "        \n",
    "        # metrics for failures template\n",
    "        if not skip_errors:\n",
    "            failure_template = get_failure_template(ground_truth_diagnosis)\n",
    "        \n",
    "            \n",
    "        # Extract generated data\n",
    "        try:\n",
    "            predicted_output = test_file[\"predicted_output\"]\n",
    "            generated_log = extract_tag_content(predicted_output, \"log\")\n",
    "            if generated_log is None:\n",
    "                generated_log = extract_tag_content(predicted_output + '</log>', \"log\")\n",
    "            if generated_log is None:\n",
    "                #print(f\"{filename}: empty log\")\n",
    "                if not skip_errors:\n",
    "                    failure_template = get_failure_template(ground_truth_diagnosis)\n",
    "                    results[filename] = failure_template\n",
    "                continue\n",
    "            df_repaired = read_csv_from_text(generated_log)\n",
    "            if list(df_repaired.columns) != ['event_id', 'case_id', 'activity', 'timestamp']:\n",
    "                #print(f\"{filename}: defective log with columns\", df_repaired.columns)\n",
    "                if not skip_errors:\n",
    "                    failure_template = get_failure_template(ground_truth_diagnosis)\n",
    "                    results[filename] = failure_template\n",
    "                continue\n",
    "            generated_diagnosis = extract_tag_content(predicted_output, \"diagnosis\")\n",
    "            if generated_diagnosis is None:\n",
    "                #print(f\"{filename}: empty diagnosis\")\n",
    "                if not skip_errors:\n",
    "                    failure_template = get_failure_template(ground_truth_diagnosis)\n",
    "                    results[filename] = failure_template\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            #print(f\"{filename}: {e}\")\n",
    "            if not skip_errors:\n",
    "                failure_template = get_failure_template(ground_truth_diagnosis)\n",
    "                results[filename] = failure_template\n",
    "            continue\n",
    "\n",
    "        # Evaluate diagnosis and repairs\n",
    "        diagnosis_metrics, patterns_injected = evaluate_diagnosis(ground_truth_diagnosis, generated_diagnosis)\n",
    "        repair_metrics = evaluate_log_repairs(df_erroneous, df_true, df_repaired, patterns_injected)\n",
    "\n",
    "        # Store results for this file\n",
    "        results[filename] = {\n",
    "            \"diagnosis\": diagnosis_metrics,\n",
    "            \"patterns_injected\": list(patterns_injected),\n",
    "            \"repair\": repair_metrics\n",
    "        }\n",
    "        #print(f\"{filename}: success\")\n",
    "    return results\n",
    "\n",
    "def initialize_results(train_dir='../data/train'):\n",
    "    \"\"\"\n",
    "    Initializes the results dictionary based on the CSV files in the given train directory.\n",
    "    \n",
    "    Parameters:\n",
    "        train_dir (str): Path to the training data directory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keys from the CSV filenames (without .csv extension) \n",
    "              and initialized structures for diagnosis and repair metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for base_file in os.listdir(train_dir):\n",
    "        full_path = os.path.join(train_dir, base_file)\n",
    "        # Only process if it's a file and ends with .csv\n",
    "        if os.path.isfile(full_path) and base_file.endswith('.csv'):\n",
    "            base_log_name = base_file.split('.csv')[0]\n",
    "            results[base_log_name] = {\n",
    "                'diagnosis': {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "                'repair': {\n",
    "                    'timestamp_deviation': {x: [] for x in range(10, 110, 10)},\n",
    "                    'timestamp_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                    'case_id_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                    'polluted_label_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                    'distorted_label_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                }\n",
    "            }\n",
    "    return results\n",
    "\n",
    "def process_evaluation_item(log_name, item, results, test_files):\n",
    "    \"\"\"\n",
    "    Processes a single evaluation result item and aggregates its metrics into the results dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        log_name (str): The name of the log file.\n",
    "        item (dict): The evaluation result for the log.\n",
    "        results (dict): The aggregated results dictionary.\n",
    "        test_files (dict): Dictionary containing test file outputs.\n",
    "    \"\"\"\n",
    "    base_log_name = log_name.split('.csv')[0]\n",
    "    \n",
    "    # Add diagnosis metrics\n",
    "    diagnosis_metrics = item['diagnosis']\n",
    "    results[base_log_name]['diagnosis']['precision'].append(diagnosis_metrics['precision'])\n",
    "    results[base_log_name]['diagnosis']['recall'].append(diagnosis_metrics['recall'])\n",
    "    results[base_log_name]['diagnosis']['f1'].append(diagnosis_metrics['f1'])\n",
    "    \n",
    "    # Extract error rates for repair metrics\n",
    "    patterns_injected = item['patterns_injected']\n",
    "    true_output = test_files[log_name]['output']\n",
    "    true_diagnosis = extract_tag_content(true_output, 'diagnosis')\n",
    "    error_to_rate = find_error_rate(true_diagnosis, patterns_injected)\n",
    "    \n",
    "    # Add repair metrics based on injected patterns and computed error rates\n",
    "    if \"form-based event capture\" in patterns_injected:\n",
    "        results[base_log_name]['repair']['timestamp_deviation'][error_to_rate['form-based event capture']].append(\n",
    "            item['repair']['timestamp_deviation']\n",
    "        )\n",
    "    if \"unanchored event\" in patterns_injected:\n",
    "        results[base_log_name]['repair']['timestamp_accuracy'][error_to_rate['unanchored event']].append(\n",
    "            item['repair']['timestamp_accuracy']\n",
    "        )\n",
    "    if \"elusive case\" in patterns_injected:\n",
    "        results[base_log_name]['repair']['case_id_accuracy'][error_to_rate['elusive case']].append(\n",
    "            item['repair']['case_id_accuracy']\n",
    "        )\n",
    "    if \"polluted label\" in patterns_injected:\n",
    "        results[base_log_name]['repair']['polluted_label_accuracy'][error_to_rate['polluted label']].append(\n",
    "            item['repair']['polluted_label_accuracy']\n",
    "        )\n",
    "    if \"distorted label\" in patterns_injected:\n",
    "        results[base_log_name]['repair']['distorted_label_accuracy'][error_to_rate['distorted label']].append(\n",
    "            item['repair']['distorted_label_accuracy']\n",
    "        )\n",
    "\n",
    "def aggregate_evaluation_results(evaluation_results, test_files, results):\n",
    "    \"\"\"\n",
    "    Iterates over evaluation results and aggregates each item into the results dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        evaluation_results (dict): A dictionary with evaluation metrics for each log.\n",
    "        test_files (dict): Dictionary containing test file outputs.\n",
    "        results (dict): The aggregated results dictionary to update.\n",
    "    \"\"\"\n",
    "    for log_name, item in evaluation_results.items():\n",
    "        process_evaluation_item(log_name, item, results, test_files)\n",
    "\n",
    "def aggregate_results(train_dir, evaluation_results, test_files):\n",
    "    \"\"\"\n",
    "    Main function to initialize results and aggregate evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        train_dir (str): The directory containing training data CSV files.\n",
    "        evaluation_results (dict): The evaluation results for each log.\n",
    "        test_files (dict): Dictionary containing test file outputs.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The aggregated results dictionary.\n",
    "    \"\"\"\n",
    "    results = initialize_results(train_dir)\n",
    "    aggregate_evaluation_results(evaluation_results, test_files, results)\n",
    "    return results\n",
    "\n",
    "def compute_median(values):\n",
    "    # If values is not iterable, wrap it in a list.\n",
    "    if not isinstance(values, (list, tuple, np.ndarray)):\n",
    "        values = [values]\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    return np.median(values)\n",
    "\n",
    "def compute_mean(values):\n",
    "    # If values is not iterable, wrap it in a list.\n",
    "    if not isinstance(values, (list, tuple, np.ndarray)):\n",
    "        values = [values]\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(values)\n",
    "\n",
    "def get_diagnosis_results(results):\n",
    "    diagnosis_rows = []\n",
    "    for log_name, log_data in results.items():\n",
    "        diag = log_data.get('diagnosis', {})\n",
    "        #precision = compute_median(diag.get('precision', []))\n",
    "        #recall    = compute_median(diag.get('recall', []))\n",
    "        #f1        = compute_median(diag.get('f1', []))\n",
    "        precision = compute_mean(diag.get('precision', []))\n",
    "        recall    = compute_mean(diag.get('recall', []))\n",
    "        f1        = compute_mean(diag.get('f1', []))\n",
    "        diagnosis_rows.append({\n",
    "            'log': log_name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "\n",
    "    df_diagnosis = pd.DataFrame(diagnosis_rows)\n",
    "    return df_diagnosis\n",
    "\n",
    "def get_repair_results(results):\n",
    "    repair_rows = []\n",
    "    repair_metric_names = [\n",
    "        'timestamp_deviation', 'timestamp_accuracy', 'case_id_accuracy', \n",
    "        'polluted_label_accuracy', 'distorted_label_accuracy', \n",
    "    ]\n",
    "\n",
    "    for log_name, log_data in results.items():\n",
    "        repair = log_data.get('repair', {})\n",
    "        # Get all unique error rates across the repair metrics\n",
    "        error_rates = set()\n",
    "        for metric in repair_metric_names:\n",
    "            error_rates.update(repair.get(metric, {}).keys())\n",
    "        # For each error rate (sorted numerically) compute the mean for each metric\n",
    "        for error_rate in sorted(error_rates, key=lambda x: int(x)):\n",
    "            row = {'log': log_name, 'error_rate': error_rate}\n",
    "            for metric in repair_metric_names:\n",
    "                values = repair.get(metric, {}).get(error_rate, [])\n",
    "                #row[metric] = compute_median(values)\n",
    "                row[metric] = compute_mean(values)\n",
    "            repair_rows.append(row)\n",
    "\n",
    "    df_repair = pd.DataFrame(repair_rows)\n",
    "    return df_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655790af-d7bc-4cf4-8eb6-451045b5792a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = \"../data/instruction/eval_predictions/individual/\"\n",
    "test_files = load_json_files(directory)\n",
    "evaluation_results = evaluate_model(test_files, skip_errors=True)\n",
    "aggregated_results = aggregate_results('../data/train', evaluation_results, test_files)\n",
    "df_diagnosis = get_diagnosis_results(aggregated_results)\n",
    "df_repair = get_repair_results(aggregated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0ddc3-8b11-4a37-88a9-7e59324b1cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demonstrate_result(file):\n",
    "    generated_output = test_files[file]['predicted_output']\n",
    "    ground_truth = test_files[file]['output']\n",
    "    \n",
    "    for tag in ['diagnosis', 'mitigation', 'log']:\n",
    "        generated_content = extract_tag_content(generated_output, tag)\n",
    "        ground_truth_content = extract_tag_content(ground_truth, tag)\n",
    "        \n",
    "        print(\"*\"*10, f\"Now comparing {tag}\", \"*\"*10)\n",
    "        print(\"\\nGround truth:\\n\", ground_truth_content)\n",
    "        print(\"\\nGenerated content:\\n\", generated_content)\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "current_sample = '2020_DomesticDeclarations.csv_batch_12_combination_32.json'\n",
    "demonstrate_result(current_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb3fc5-236a-4936-ab0f-593e3ec4688b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_diagnosis.sort_values(by='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aafe5d-a1db-471f-b12a-01c921bbab42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(df_repair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41276a47-c982-4ab9-8e25-b4ef62b1f30b",
   "metadata": {},
   "source": [
    "# No Repair Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638c5a9-d825-4f5a-afd1-9d71bd2c98fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def load_json_files(directory):\n",
    "    \"\"\"Load all JSON files from a directory and return a dictionary with filenames as keys.\"\"\"\n",
    "    test_files = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                test_files[filename] = json.load(file)  # Store data with filename as key\n",
    "    return test_files\n",
    "\n",
    "def extract_tag_content(text, tag):\n",
    "    \"\"\"Extracts the substring between the last occurrence of <tag> and </tag> from the given text.\"\"\"\n",
    "    pattern = fr'<{tag}>(.*?)</{tag}>'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[-1].strip() if matches else None\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing punctuation and converting to lowercase.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text).lower()\n",
    "\n",
    "def find_patterns(text, patterns):\n",
    "    \"\"\"\n",
    "    Find which patterns are in the text, ignoring case and punctuation.\n",
    "    Returns a list of matched patterns.\n",
    "    \"\"\"\n",
    "    normalized_text = normalize_text(text)\n",
    "    matched_patterns = [pattern for pattern in patterns if normalize_text(pattern) in normalized_text]\n",
    "    return matched_patterns\n",
    "\n",
    "def find_error_rate(text, patterns):\n",
    "    \"\"\"Find error rates for given patterns in the text.\"\"\"\n",
    "    #normalized_text = normalize_text(text)\n",
    "    results = {}\n",
    "    for pattern in patterns:\n",
    "        normalized_pattern = normalize_text(pattern)\n",
    "        match = re.search(fr'{normalized_pattern}.*?error rate of (\\d+)%', text)\n",
    "        if match:\n",
    "            results[pattern] = int(match.group(1))\n",
    "    return results\n",
    "\n",
    "def read_csv_from_text(text):\n",
    "    data = StringIO(text)\n",
    "    df = pd.read_csv(data)\n",
    "    return df\n",
    "\n",
    "def timestamp_deviation(df_erroneous, df_true, df_repaired):\n",
    "    \"\"\"Calculate the mean deviation of repaired timestamps from ground truth in seconds (only for erroneous events).\"\"\"\n",
    "    merged_err = df_erroneous.merge(df_true, on=\"event_id\", suffixes=(\"_err\", \"_true\"))\n",
    "    erroneous_events = merged_err[merged_err[\"timestamp_err\"] != merged_err[\"timestamp_true\"]][\"event_id\"]\n",
    "    df_true_filtered = df_true[df_true[\"event_id\"].isin(erroneous_events)]\n",
    "    df_repaired_filtered = df_repaired[df_repaired[\"event_id\"].isin(erroneous_events)]\n",
    "    merged = df_true_filtered.merge(df_repaired_filtered, on=\"event_id\", suffixes=(\"_true\", \"_repaired\"))\n",
    "    merged[\"timestamp_true\"] = pd.to_datetime(merged[\"timestamp_true\"], errors='coerce')\n",
    "    merged[\"timestamp_repaired\"] = pd.to_datetime(merged[\"timestamp_repaired\"], errors='coerce')\n",
    "    merged[\"deviation\"] = (merged[\"timestamp_repaired\"] - merged[\"timestamp_true\"]).dt.total_seconds().abs()\n",
    "    return merged[\"deviation\"].mean()\n",
    "\n",
    "def evaluate_log_repairs(df_erroneous, df_true, df_repaired, true_patterns):\n",
    "    \"\"\"Evaluate how well the log was repaired based on detected patterns.\"\"\"\n",
    "    metrics = {}\n",
    "    if \"form-based event capture\" in true_patterns:\n",
    "        metrics[\"timestamp_deviation\"] = timestamp_deviation(df_erroneous, df_true, df_repaired)\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(test_files, skip_errors=True):\n",
    "    \"\"\"Run evaluation on all test files (provided as a dict keyed by filename) and return a dict with filenames as keys.\"\"\"\n",
    "    results = {}\n",
    "    for filename, test_file in test_files.items():\n",
    "        # Extract ground truth data\n",
    "        erroneous_log = test_file[\"input\"]\n",
    "        df_erroneous = read_csv_from_text(erroneous_log)\n",
    "        ground_truth_log = extract_tag_content(test_file[\"output\"], \"log\")\n",
    "        df_true = read_csv_from_text(ground_truth_log)\n",
    "        ground_truth_diagnosis = extract_tag_content(test_file[\"output\"], \"diagnosis\")\n",
    "        \n",
    "        patterns = [\"form-based event capture\", \"elusive case\", \"polluted label\", \"distorted label\", \"unanchored event\"]\n",
    "        patterns_injected = set(find_patterns(ground_truth_diagnosis, patterns))\n",
    "        \n",
    "        # Extract generated data\n",
    "        try:\n",
    "            predicted_output = test_file[\"predicted_output\"]\n",
    "            generated_log = extract_tag_content(predicted_output, \"log\")\n",
    "            df_repaired = read_csv_from_text(generated_log)\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(f\"{filename}: {e}\")\n",
    "            if not skip_errors:\n",
    "                failure_template = get_failure_template(ground_truth_diagnosis)\n",
    "                results[filename] = failure_template\n",
    "            continue\n",
    "\n",
    "        # Evaluate repair\n",
    "        repair_metrics = evaluate_log_repairs(df_erroneous, df_true, df_repaired, patterns_injected)\n",
    "\n",
    "        # Store results for this file\n",
    "        results[filename] = {\n",
    "            \"patterns_injected\": list(patterns_injected),\n",
    "            \"repair\": repair_metrics\n",
    "        }\n",
    "        #print(f\"{filename}: success\")\n",
    "    return results\n",
    "\n",
    "def initialize_results(train_dir='../data/train'):\n",
    "    \"\"\"\n",
    "    Initializes the results dictionary based on the CSV files in the given train directory.\n",
    "    \n",
    "    Parameters:\n",
    "        train_dir (str): Path to the training data directory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keys from the CSV filenames (without .csv extension) \n",
    "              and initialized structures for diagnosis and repair metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for base_file in os.listdir(train_dir):\n",
    "        full_path = os.path.join(train_dir, base_file)\n",
    "        # Only process if it's a file and ends with .csv\n",
    "        if os.path.isfile(full_path) and base_file.endswith('.csv'):\n",
    "            base_log_name = base_file.split('.csv')[0]\n",
    "            results[base_log_name] = {\n",
    "                'repair': {\n",
    "                    'timestamp_deviation': {x: [] for x in range(10, 110, 10)},\n",
    "                    'timestamp_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                    'case_id_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                    'polluted_label_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                    'distorted_label_accuracy': {x: [] for x in range(10, 110, 10)},\n",
    "                }\n",
    "            }\n",
    "    return results\n",
    "\n",
    "def process_evaluation_item(log_name, item, results, test_files):\n",
    "    \"\"\"\n",
    "    Processes a single evaluation result item and aggregates its metrics into the results dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        log_name (str): The name of the log file.\n",
    "        item (dict): The evaluation result for the log.\n",
    "        results (dict): The aggregated results dictionary.\n",
    "        test_files (dict): Dictionary containing test file outputs.\n",
    "    \"\"\"\n",
    "    base_log_name = log_name.split('.csv')[0]\n",
    "    \n",
    "    \n",
    "    # Extract error rates for repair metrics\n",
    "    patterns_injected = item['patterns_injected']\n",
    "    true_output = test_files[log_name]['output']\n",
    "    true_diagnosis = extract_tag_content(true_output, 'diagnosis')\n",
    "    error_to_rate = find_error_rate(true_diagnosis, patterns_injected)\n",
    "    \n",
    "    # Add repair metrics based on injected patterns and computed error rates\n",
    "    if \"form-based event capture\" in patterns_injected:\n",
    "        results[base_log_name]['repair']['timestamp_deviation'][error_to_rate['form-based event capture']].append(\n",
    "            item['repair']['timestamp_deviation']\n",
    "        )\n",
    "\n",
    "def aggregate_evaluation_results(evaluation_results, test_files, results):\n",
    "    \"\"\"\n",
    "    Iterates over evaluation results and aggregates each item into the results dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        evaluation_results (dict): A dictionary with evaluation metrics for each log.\n",
    "        test_files (dict): Dictionary containing test file outputs.\n",
    "        results (dict): The aggregated results dictionary to update.\n",
    "    \"\"\"\n",
    "    for log_name, item in evaluation_results.items():\n",
    "        process_evaluation_item(log_name, item, results, test_files)\n",
    "\n",
    "def aggregate_results(train_dir, evaluation_results, test_files):\n",
    "    \"\"\"\n",
    "    Main function to initialize results and aggregate evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        train_dir (str): The directory containing training data CSV files.\n",
    "        evaluation_results (dict): The evaluation results for each log.\n",
    "        test_files (dict): Dictionary containing test file outputs.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The aggregated results dictionary.\n",
    "    \"\"\"\n",
    "    results = initialize_results(train_dir)\n",
    "    aggregate_evaluation_results(evaluation_results, test_files, results)\n",
    "    return results\n",
    "\n",
    "def compute_median(values):\n",
    "    # If values is not iterable, wrap it in a list.\n",
    "    if not isinstance(values, (list, tuple, np.ndarray)):\n",
    "        values = [values]\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    return np.median(values)\n",
    "\n",
    "def compute_mean(values):\n",
    "    # If values is not iterable, wrap it in a list.\n",
    "    if not isinstance(values, (list, tuple, np.ndarray)):\n",
    "        values = [values]\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(values)\n",
    "\n",
    "def get_repair_results(results):\n",
    "    repair_rows = []\n",
    "    repair_metric_names = [\n",
    "        'timestamp_deviation', 'timestamp_accuracy', 'case_id_accuracy', \n",
    "        'polluted_label_accuracy', 'distorted_label_accuracy', \n",
    "    ]\n",
    "\n",
    "    for log_name, log_data in results.items():\n",
    "        repair = log_data.get('repair', {})\n",
    "        # Get all unique error rates across the repair metrics\n",
    "        error_rates = set()\n",
    "        for metric in repair_metric_names:\n",
    "            error_rates.update(repair.get(metric, {}).keys())\n",
    "        # For each error rate (sorted numerically) compute the mean for each metric\n",
    "        for error_rate in sorted(error_rates, key=lambda x: int(x)):\n",
    "            row = {'log': log_name, 'error_rate': error_rate}\n",
    "            for metric in repair_metric_names:\n",
    "                values = repair.get(metric, {}).get(error_rate, [])\n",
    "                #row[metric] = compute_median(values)\n",
    "                row[metric] = compute_mean(values)\n",
    "            repair_rows.append(row)\n",
    "\n",
    "    df_repair = pd.DataFrame(repair_rows)\n",
    "    return df_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbe88c-5098-44aa-84e3-aa9af38958e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data/instruction/eval_predictions/individual/\"\n",
    "test_files = load_json_files(directory)\n",
    "\n",
    "# for no repair overwrite generated output with input\n",
    "for key in test_files.keys():\n",
    "    test_files[key]['predicted_output'] = '<log>' + test_files[key]['input'] + '</log>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1df8d-a59d-4723-86ee-567c39574b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_model(test_files, skip_errors=False)\n",
    "aggregated_results = aggregate_results('../data/train', evaluation_results, test_files)\n",
    "df_repair_no_repair = get_repair_results(aggregated_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185dca19-c7cd-421c-b1fe-d8bde4b8862c",
   "metadata": {},
   "source": [
    "# Visualization - Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be782c0-25b0-4a31-9f5a-c3bf67e84de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})  # Adjust the number to increase or decrease font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84052732-e031-4ffa-ba21-96a28da365ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "log_names = {\n",
    "    '2020_RequestForPayment' : 'BPIC2020 Request for Payment',\n",
    "    '2020_PermitLog' : 'BPIC2020 Permit Log',\n",
    "    '2020_DomesticDeclarations' : 'BPIC2020 Domestic Declarations',\n",
    "    '2020_InternationalDeclarations' : 'BPIC2020 International Declarations',\n",
    "    '2020_PrepaidTravelCost' : 'BPIC2020 Prepaid Travel Cost',\n",
    "    '2012_BPI_Challenge' : 'BPIC2012',\n",
    "    '2018_BPI_Challenge' : 'BPIC2018',\n",
    "    '2019_BPI_Challenge' : 'BPIC2019',\n",
    "}\n",
    "\n",
    "accuracy_metrics = ['timestamp_accuracy', 'case_id_accuracy', 'polluted_label_accuracy', 'distorted_label_accuracy']\n",
    "logs = df_repair['log'].unique()\n",
    "num_logs = len(logs)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(20, 15))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Collect handles and labels for the legend\n",
    "handles, labels = [], []\n",
    "collected_metrics = set()\n",
    "\n",
    "for ax, log in zip(axs, logs):\n",
    "    df_log = df_repair[df_repair['log'] == log]\n",
    "    for metric in accuracy_metrics:\n",
    "        line, = ax.plot(df_log['error_rate'], df_log[metric]*100, marker='o', label=metric)\n",
    "        if metric not in collected_metrics:\n",
    "            handles.append(line)\n",
    "            labels.append(metric.replace('_', ' ').title())\n",
    "            collected_metrics.add(metric)\n",
    "    \n",
    "    ax.set_title(f'{log_names[log]}')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_xticks(df_log['error_rate'].unique())\n",
    "    ax.set_yticks(df_log['error_rate'].unique())\n",
    "    ax.tick_params(axis='x', labelrotation=45, labelbottom=True)\n",
    "    ax.set_xlim(5, 105)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Remove content from the last unused subplot and place the legend there\n",
    "empty_ax = axs[-1]\n",
    "empty_ax.axis('off')\n",
    "empty_ax.legend(handles, labels, loc='center', fontsize=16)\n",
    "\n",
    "#fig.suptitle('Accuracy Metrics Across Test Logs', fontsize=16)\n",
    "fig.text(0.5, 0.04, 'Error Rate (%)', ha='center', va='center', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "\n",
    "plt.savefig('../models/OTRTA_v7_instruct_base_new_system/accuracy_metrics.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c6c33-5d44-48ea-aaf9-967f1a777e57",
   "metadata": {},
   "source": [
    "# Timestamp Deviation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b17ce-7bca-4a12-af0c-f94b665cc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assume df_repair, df_repair_no_repair, and log_names are already available\n",
    "logs = df_repair['log'].unique()\n",
    "num_logs = len(logs)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(20, 15))\n",
    "axs = axs.flatten()\n",
    "\n",
    "handles, labels = [], []\n",
    "collected = set()\n",
    "\n",
    "for ax, log in zip(axs, logs):\n",
    "    df_log = df_repair[df_repair['log'] == log]\n",
    "    df_baseline = df_repair_no_repair[df_repair_no_repair['log'] == log]\n",
    "    \n",
    "    # Convert deviation from seconds to hours\n",
    "    line1, = ax.plot(df_log['error_rate'], df_log['timestamp_deviation'] / 3600, marker='o', label='Timestamp Deviation after Repair')\n",
    "    line2, = ax.plot(df_baseline['error_rate'], df_baseline['timestamp_deviation'] / 3600, marker='o', label='Timestamp Deviation No Repair')\n",
    "    \n",
    "    if 'Timestamp Deviation after Repair' not in collected:\n",
    "        handles.append(line1)\n",
    "        labels.append('Timestamp Deviation after Repair')\n",
    "        collected.add('Timestamp Deviation after Repair')\n",
    "    if 'Timestamp Deviation without Repair' not in collected:\n",
    "        handles.append(line2)\n",
    "        labels.append('Timestamp Deviation without Repair')\n",
    "        collected.add('Timestamp Deviation without Repair')\n",
    "    \n",
    "    title = log_names.get(log, log)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Mean Deviation (hours)')\n",
    "    ax.set_xticks(df_log['error_rate'].unique())\n",
    "    ax.tick_params(axis='x', labelrotation=45, labelbottom=True)\n",
    "    ax.set_xlim(5, 105)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Remove content from the last unused subplot and place legend there\n",
    "empty_ax = axs[-1]\n",
    "empty_ax.axis('off')\n",
    "empty_ax.legend(handles, labels, loc='center', fontsize=16)\n",
    "\n",
    "#fig.suptitle('Timestamp Deviation Comparison (in Hours) Across Test Logs', fontsize=16)\n",
    "fig.text(0.5, 0.04, 'Error Rate (%)', ha='center', va='center', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "\n",
    "plt.savefig('../models/OTRTA_v7_instruct_base_new_system/non_accuracy_metrics.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1983ff-2bfb-46ec-877a-db96e956ddcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
